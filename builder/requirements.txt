scipy==1.14.0
numpy==1.26.4 
pandas

torch==2.4.0+cu121
torchvision==0.19.0+cu121
torchaudio==2.4.0+cu121
--extra-index-url https://download.pytorch.org/whl/cu121

flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0.post2/flash_attn-2.8.0.post2+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

vector-quantize-pytorch==1.14.2
peft==0.15.1
transformers>=4.45.1,<4.47.0
xfuser==0.4.1
ninja

ftfy
einops
dacite
json_repair
pydantic
loguru
ruff==0.12.2
click
tqdm
omegaconf

runpod
boto3==1.35.36
s3fs

librosa==0.10.2.post1
pydub
descript-audio-codec

Pillow
ffmpeg-python
imageio
imageio-ffmpeg