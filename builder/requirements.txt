tqdm
librosa==0.10.2.post1
peft==0.15.1
transformers>=4.45.1,<4.47.0
scipy==1.14.0
numpy==1.26.4 
xfuser==0.4.1
ftfy
einops
omegaconf
torchvision
ninja
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu118torch2.1cxx11abiTRUE-cp310-cp310-linux_x86_64.whl

dacite
boto3==1.35.36
s3fs
json_repair
pandas
pydantic
vector-quantize-pytorch==1.14.2
loguru
pydub
ruff==0.12.2
click
torchaudio
descript-audio-codec

runpod
Pillow
ffmpeg-python
imageio
imageio-ffmpeg